# Reinforcement Learning

## Slides

- [强化学习引入](slides/1.强化学习引入.pdf)
- [蒙特卡洛方法与时序差分算法](slides/强化学习-合%281%29.pdf)
- [DQN](slides/DQN.pdf)
- [Deepseek-R1](slides/deepseek-r1.pdf)
- [过程奖励模型PRM](slides/20250521-PRM%281%29.pdf)

## Books

- [深度强化学习：基础、研究与应用](https://deepreinforcementlearningbook.org/assets/pdfs/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%28%E4%B8%AD%E6%96%87%E7%89%88-%E5%BD%A9%E8%89%B2%E5%8E%8B%E7%BC%A9%29.pdf) 
- [强化学习 (by Sutton)](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)

## Papers

#### RLHF & RLAIF

- [InstructGPT：Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)
- [RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267)

#### Deepseek-R1 & Process Reward Model

- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)
- [Let's Verify Step by Step](https://arxiv.org/pdf/2305.20050)

#### DPO

- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290)

#### GRPO & ORPO

- [(GRPO) DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/pdf/2402.03300)
- [ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/pdf/2403.07691)
