# Reinforcement Learning

## Slides

- [强化学习引入](slides/1.强化学习引入.pdf)
- [蒙特卡洛与时序差分](slides/强化学习-合%281%29.pdf)
- [DQN](slides/DQN.pdf)
- [Deepseek-R1 & DAPO](slides/deepseek-r1.pdf)
- [过程奖励模型PRM](slides/20250521-PRM%281%29.pdf)
- [强化学习面临的挑战](slides/Taxonomy_of_RL_Algorithms.pdf)
- [DDPG](slides/Pre_4_9.pdf)
- [RLHF](slides/InstructGPT.pdf)
- [GRPO](slides/20250430 GRPO.pdf)

## Books

- [深度强化学习：基础、研究与应用](https://deepreinforcementlearningbook.org/assets/pdfs/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%28%E4%B8%AD%E6%96%87%E7%89%88-%E5%BD%A9%E8%89%B2%E5%8E%8B%E7%BC%A9%29.pdf) 
- [强化学习 (by Sutton)](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)

## Papers

#### RLHF & RLAIF

- [InstructGPT：Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)
- [RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267)

#### Deepseek-R1

- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948)

#### Process Reward Model

- [Let's Verify Step by Step](https://arxiv.org/pdf/2305.20050)

#### DPO

- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290)
- [ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/pdf/2403.07691)

#### GRPO

- [(GRPO) DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/pdf/2402.03300)
- [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/pdf/2503.14476)

#### Post-training & Test-Time Scaling Law

- [Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/pdf/2408.03314v1)
- [s1: Simple test-time scaling](https://arxiv.org/pdf/2501.19393)
